{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import Error\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from faker import Faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_json(data, filename):\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "\n",
    "def read_from_json(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to PostgreSQL\n"
     ]
    }
   ],
   "source": [
    "# Connection string\n",
    "conn_string = os.getenv(\"DATABASE_URL\")\n",
    "\n",
    "try:\n",
    "    # Connect to PostgreSQL database\n",
    "    connection = psycopg2.connect(conn_string)\n",
    "    print(\"Connected to PostgreSQL\")\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "except (Exception, Error) as error:\n",
    "    print(\"Error while connecting to PostgreSQL:\", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder 'data' already exists.\n"
     ]
    }
   ],
   "source": [
    "def create_folder(folder_name):\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "        print(f\"Folder '{folder_name}' created.\")\n",
    "    else:\n",
    "        print(f\"Folder '{folder_name}' already exists.\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "folder_name = \"data\"\n",
    "create_folder(folder_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Inserting Organization Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORG_ID = \"89933\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "organization_data_list = [\n",
    "    {\n",
    "        \"id\": ORG_ID,\n",
    "        \"name\": \"Tech Innovations Inc.\",\n",
    "        \"contact_email\": \"contact@techinnovations.com\",\n",
    "        \"phone_number\": \"+1-555-0123\",\n",
    "        \"address\": \"123 Tech Avenue, Silicon Valley, CA\",\n",
    "        \"industry\": \"Information Technology and Services\",\n",
    "        \"founded_year\": 2010,\n",
    "        \"number_of_employees\": 250,\n",
    "        \"website\": \"www.techinnovations.com\",\n",
    "    }\n",
    "    # Add more organization data dictionaries as needed\n",
    "]\n",
    "\n",
    "# Write organization data to JSON file\n",
    "write_to_json(organization_data_list, f\"{folder_name}/organization_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Organization 'Tech Innovations Inc.' inserted with ID: 8fd51c1d-32f9-48e3-a473-cf57f29bea05\n"
     ]
    }
   ],
   "source": [
    "connection = psycopg2.connect(conn_string)\n",
    "cursor = connection.cursor()\n",
    "try:\n",
    "\n",
    "    # Read organization data from JSON file\n",
    "    organization_data_list = read_from_json(f\"{folder_name}/organization_data.json\")\n",
    "\n",
    "    # Define the INSERT query\n",
    "    insert_query = \"\"\"\n",
    "        INSERT INTO \"Organization\" (name, contact_email, phone_number)\n",
    "        VALUES (%s, %s, %s)\n",
    "        RETURNING id\n",
    "    \"\"\"\n",
    "\n",
    "    # Iterate over organization data list and insert each entry\n",
    "    for organization_data in organization_data_list:\n",
    "        # Execute the INSERT query\n",
    "        cursor.execute(\n",
    "            insert_query,\n",
    "            (\n",
    "                organization_data[\"name\"],\n",
    "                organization_data[\"contact_email\"],\n",
    "                organization_data[\"phone_number\"],\n",
    "            ),\n",
    "        )\n",
    "        organization_id = cursor.fetchone()[\n",
    "            0\n",
    "        ]  # Get the ID of the inserted organization\n",
    "        print(\n",
    "            f\"Organization '{organization_data['name']}' inserted with ID: {organization_id}\"\n",
    "        )\n",
    "\n",
    "    # Commit the transaction\n",
    "    connection.commit()\n",
    "except (Exception, Error) as error:\n",
    "    print(\"Error while connecting to PostgreSQL:\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organization 'Tech Innovations Inc.' inserted with ID: 8fd51c1d-32f9-48e3-a473-cf57f29bea05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inserting Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Faker\n",
    "fake = Faker()\n",
    "number_of_locations = 20\n",
    "\n",
    "\n",
    "# Generate fake location data and write to JSON file\n",
    "def generate_location_data(org_id, number_of_locations):\n",
    "    locations = []\n",
    "    for _ in range(number_of_locations):\n",
    "        location = {\n",
    "            \"id\": str(fake.unique.random_number(digits=5)),\n",
    "            \"org_id\": org_id,\n",
    "            \"name\": fake.street_name(),\n",
    "            \"description\": (\n",
    "                fake.text(max_nb_chars=200)\n",
    "                if fake.boolean(chance_of_getting_true=50)\n",
    "                else None\n",
    "            ),\n",
    "            \"latitude\": float(fake.latitude()),\n",
    "            \"longitude\": float(fake.longitude()),\n",
    "        }\n",
    "        locations.append(location)\n",
    "\n",
    "    write_to_json(locations, f\"{folder_name}/locations.json\")\n",
    "\n",
    "\n",
    "# Generate and write fake location data to JSON\n",
    "generate_location_data(ORG_ID, number_of_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 20 fake Location entries into the database.\n"
     ]
    }
   ],
   "source": [
    "# Now read the data from the JSON file and insert it into the database\n",
    "def insert_locations_from_json(cursor, json_file):\n",
    "    locations = read_from_json(f\"{folder_name}/locations.json\")\n",
    "\n",
    "    insert_query = \"\"\"\n",
    "        INSERT INTO \"Location\" ( org_id, name, description, latitude, longitude)\n",
    "        VALUES (%s, %s, %s, %s, %s);\n",
    "    \"\"\"\n",
    "\n",
    "    for location in locations:\n",
    "        cursor.execute(\n",
    "            insert_query,\n",
    "            (\n",
    "                location[\"org_id\"],\n",
    "                location[\"name\"],\n",
    "                location[\"description\"],\n",
    "                location[\"latitude\"],\n",
    "                location[\"longitude\"],\n",
    "            ),\n",
    "        )\n",
    "\n",
    "\n",
    "# Set up database connection\n",
    "connection_string = os.getenv(\"DATABASE_URL\")\n",
    "conn = psycopg2.connect(connection_string)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Read from the JSON file and insert data into PostgreSQL\n",
    "insert_locations_from_json(cursor, \"locations.json\")\n",
    "\n",
    "# Commit the transaction\n",
    "conn.commit()\n",
    "\n",
    "# Clean up\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(f\"Inserted {number_of_locations} fake Location entries into the database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to be done for uuid\n",
    "ALTER TABLE \"Location\" ALTER COLUMN id SET DEFAULT uuid_generate_v4();\n",
    "CREATE EXTENSION IF NOT EXISTS \"uuid-ossp\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organization 'Tech Innovations Inc.' inserted with ID: 8fd51c1d-32f9-48e3-a473-cf57f29bea05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Inserting Skill Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON data written to data/skill_categories.json\n"
     ]
    }
   ],
   "source": [
    "from faker import Faker\n",
    "import json\n",
    "\n",
    "# Create a Faker instance\n",
    "faker = Faker()\n",
    "\n",
    "# Define the static organization ID\n",
    "org_id = ORG_ID\n",
    "\n",
    "# Skill category names and descriptions\n",
    "skill_categories_info = [\n",
    "    (\n",
    "        \"Senior Management\",\n",
    "        \"Skills relevant to leadership and high-level management decisions.\",\n",
    "    ),\n",
    "    (\n",
    "        \"Cybersecurity\",\n",
    "        \"Skills related to protecting information from unauthorized access and cyber threats.\",\n",
    "    ),\n",
    "    (\n",
    "        \"Software Engineering\",\n",
    "        \"Skills in designing, developing, testing, and evaluating software systems.\",\n",
    "    ),\n",
    "    (\n",
    "        \"Project Management\",\n",
    "        \"Skills required for effectively managing projects and resources.\",\n",
    "    ),\n",
    "    (\n",
    "        \"Data Science\",\n",
    "        \"Skills for analyzing complex data to extract actionable insights.\",\n",
    "    ),\n",
    "    (\n",
    "        \"DevOps\",\n",
    "        \"Skills for collaboration between software development and IT operations.\",\n",
    "    ),\n",
    "    (\n",
    "        \"Cloud Computing\",\n",
    "        \"Skills related to the delivery of computing services over the internet.\",\n",
    "    ),\n",
    "    (\n",
    "        \"Front End Technologies\",\n",
    "        \"Skills in client-side development, such as using Angular, React JS, and Vue.\",\n",
    "    ),\n",
    "    (\n",
    "        \"Server Side Technologies\",\n",
    "        \"Skills in server-side development, such as Node JS, Java Spring Boot, .NET, Laravel, Adobe Cold fusion.\",\n",
    "    ),\n",
    "    (\"Database Technologies\", \"Skills related to database management, such as SQL.\"),\n",
    "    (\n",
    "        \"Business Intelligence\",\n",
    "        \"Skills in BI tools like Apache Superset, Power BI, Tableau, Looker.\",\n",
    "    ),\n",
    "    (\n",
    "        \"User Interface Design\",\n",
    "        \"Skills in UI design and development, such as Wordpress, Web Designing (HTML, CSS, SCSS), Javascript (ES6).\",\n",
    "    ),\n",
    "    (\n",
    "        \"Mobile Development\",\n",
    "        \"Skills in mobile app development, such as Flutter and React Native.\",\n",
    "    ),\n",
    "    (\"Web Scraping\", \"Skills in scraping technologies like Selenium / Puppeteer.\"),\n",
    "    (\"Scripting Languages\", \"Skills in scripting languages, such as Python.\"),\n",
    "    (\n",
    "        \"ELT / ETL Technologies\",\n",
    "        \"Skills in ELT/ETL processes, such as DBT, Alteryx, Azure Synapse, Databricks (Spark), Matillion.\",\n",
    "    ),\n",
    "    (\"Data Warehousing\", \"Skills related to data warehousing, such as Snowflake.\"),\n",
    "    (\n",
    "        \"Machine Learning\",\n",
    "        \"Skills in ML technologies like opencv, scikit-learn, tensorflow, neural networks.\",\n",
    "    ),\n",
    "    (\"Natural Language Processing\", \"Skills in NLP and sentimental analysis.\"),\n",
    "    (\n",
    "        \"Cloud Operations\",\n",
    "        \"Skills in cloud operations, including Cloud Native (Containerization & Orchestration), Azure Logic Apps, Microsoft Flow, Serverless.\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Generate SkillCategory entries with Faker-generated IDs\n",
    "skill_categories = [\n",
    "    {\n",
    "        \"id\": i,  # Auto-incrementing ID, padded to 5 digits\n",
    "        \"org_id\": org_id,\n",
    "        \"name\": name,\n",
    "        \"description\": description,\n",
    "    }\n",
    "    for i, (name, description) in enumerate(skill_categories_info, start=1)\n",
    "]\n",
    "\n",
    "# Define the folder name where the file will be saved\n",
    "\n",
    "# Write the JSON string to a file\n",
    "json_filename = f\"{folder_name}/skill_categories.json\"\n",
    "\n",
    "# Write the JSON data to a file\n",
    "with open(json_filename, \"w\") as file:\n",
    "    json.dump(skill_categories, file, indent=2)\n",
    "\n",
    "print(f\"JSON data written to {json_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 13 SkillCategory entries into the database.\n"
     ]
    }
   ],
   "source": [
    "# Function to read data from a JSON file\n",
    "def read_from_json(json_file_path):\n",
    "    with open(json_file_path, \"r\") as file:\n",
    "        return json.load(file)\n",
    "\n",
    "\n",
    "# Now read the data from the JSON file and insert it into the database\n",
    "def insert_skill_categories_from_json(cursor, json_data):\n",
    "    insert_query = \"\"\"\n",
    "        INSERT INTO \"SkillCategory\" (id, org_id, name, description)\n",
    "        VALUES (%s, %s, %s, %s);\n",
    "    \"\"\"\n",
    "\n",
    "    for skill_category in json_data:\n",
    "        cursor.execute(\n",
    "            insert_query,\n",
    "            (\n",
    "                skill_category[\"id\"],\n",
    "                skill_category[\"org_id\"],\n",
    "                skill_category[\"name\"],\n",
    "                skill_category[\"description\"],\n",
    "            ),\n",
    "        )\n",
    "\n",
    "\n",
    "# Function to connect to the database and insert data\n",
    "# Read from the JSON file\n",
    "skill_categories_json_path = f\"{folder_name}/skill_categories.json\"\n",
    "skill_categories = read_from_json(skill_categories_json_path)\n",
    "\n",
    "# Set up database connection using environment variables\n",
    "connection_string = os.getenv(\"DATABASE_URL\")\n",
    "conn = psycopg2.connect(connection_string)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Insert data into PostgreSQL\n",
    "insert_skill_categories_from_json(cursor, skill_categories)\n",
    "\n",
    "# Commit the transaction\n",
    "conn.commit()\n",
    "\n",
    "# Clean up\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(f\"Inserted {len(skill_categories)} SkillCategory entries into the database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Inserting Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/skills.json'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming the skill categories JSON is stored in 'skill_categories.json'\n",
    "skill_categories_file = f\"{folder_name}/skill_categories.json\"\n",
    "\n",
    "\n",
    "# Load the skill categories from the JSON file to get the real UUIDs\n",
    "with open(skill_categories_file, \"r\") as file:\n",
    "    skill_categories_data = json.load(file)\n",
    "    category_id_map = {\n",
    "        category[\"name\"]: category[\"id\"] for category in skill_categories_data\n",
    "    }\n",
    "\n",
    "# Extended list of skills for each category with at least four skills per category\n",
    "skills_info = [\n",
    "    # Senior Management\n",
    "    (\n",
    "        \"Corporate governance\",\n",
    "        \"Understanding and applying the best practices for corporate governance.\",\n",
    "        category_id_map[\"Senior Management\"],\n",
    "    ),\n",
    "    (\n",
    "        \"Strategic planning\",\n",
    "        \"Developing long-term strategies for the organization's growth.\",\n",
    "        category_id_map[\"Senior Management\"],\n",
    "    ),\n",
    "    (\n",
    "        \"Visionary leadership\",\n",
    "        \"Providing direction and inspiring the organization to achieve its vision.\",\n",
    "        category_id_map[\"Senior Management\"],\n",
    "    ),\n",
    "    (\n",
    "        \"Financial acumen\",\n",
    "        \"Understanding and applying financial principles to drive business success.\",\n",
    "        category_id_map[\"Senior Management\"],\n",
    "    ),\n",
    "    # Cybersecurity\n",
    "    (\n",
    "        \"Network security\",\n",
    "        \"Protecting computer networks from intrusions and attacks.\",\n",
    "        category_id_map[\"Cybersecurity\"],\n",
    "    ),\n",
    "    (\n",
    "        \"Information security\",\n",
    "        \"Ensuring the confidentiality, integrity, and availability of data.\",\n",
    "        category_id_map[\"Cybersecurity\"],\n",
    "    ),\n",
    "    (\n",
    "        \"Cybersecurity policies\",\n",
    "        \"Developing and enforcing policies to protect against cyber threats.\",\n",
    "        category_id_map[\"Cybersecurity\"],\n",
    "    ),\n",
    "    (\n",
    "        \"Incident response\",\n",
    "        \"Responding to and recovering from security breaches and incidents.\",\n",
    "        category_id_map[\"Cybersecurity\"],\n",
    "    ),\n",
    "    # Software Engineering\n",
    "    (\n",
    "        \"System design\",\n",
    "        \"Architecting complex software systems to meet business requirements.\",\n",
    "        category_id_map[\"Software Engineering\"],\n",
    "    ),\n",
    "    (\n",
    "        \"Coding best practices\",\n",
    "        \"Maintaining high standards of coding and software development.\",\n",
    "        category_id_map[\"Software Engineering\"],\n",
    "    ),\n",
    "    (\n",
    "        \"Software testing\",\n",
    "        \"Designing and executing tests to ensure software quality.\",\n",
    "        category_id_map[\"Software Engineering\"],\n",
    "    ),\n",
    "    (\n",
    "        \"Continuous integration\",\n",
    "        \"Automating the integration of code changes from multiple contributors.\",\n",
    "        category_id_map[\"Software Engineering\"],\n",
    "    ),\n",
    "    # Project Management\n",
    "    (\n",
    "        \"Resource allocation\",\n",
    "        \"Effectively distributing resources across projects to ensure efficient use.\",\n",
    "        category_id_map[\"Project Management\"],\n",
    "    ),\n",
    "    (\n",
    "        \"Risk assessment\",\n",
    "        \"Identifying potential risks in project planning and execution.\",\n",
    "        category_id_map[\"Project Management\"],\n",
    "    ),\n",
    "    (\n",
    "        \"Project scheduling\",\n",
    "        \"Planning and organizing project tasks and timelines.\",\n",
    "        category_id_map[\"Project Management\"],\n",
    "    ),\n",
    "    (\n",
    "        \"Stakeholder management\",\n",
    "        \"Managing relationships with all project stakeholders.\",\n",
    "        category_id_map[\"Project Management\"],\n",
    "    ),\n",
    "    # Data Science\n",
    "    (\n",
    "        \"Data Analysis\",\n",
    "        \"Analyzing and interpreting complex datasets to extract meaningful insights.\",\n",
    "        category_id_map[\"Data Science\"],\n",
    "    ),\n",
    "    (\n",
    "        \"Machine Learning\",\n",
    "        \"Applying statistical models and algorithms to data to predict outcomes.\",\n",
    "        category_id_map[\"Data Science\"],\n",
    "    ),\n",
    "    (\n",
    "        \"Data visualization\",\n",
    "        \"Representing data in graphical format to aid understanding.\",\n",
    "        category_id_map[\"Data Science\"],\n",
    "    ),\n",
    "    (\n",
    "        \"Big Data technologies\",\n",
    "        \"Utilizing technologies for processing large datasets.\",\n",
    "        category_id_map[\"Data Science\"],\n",
    "    ),\n",
    "    # DevOps\n",
    "    (\n",
    "        \"Continuous Integration/Continuous Deployment\",\n",
    "        \"Implementing CI/CD pipelines for software delivery.\",\n",
    "        category_id_map[\"DevOps\"],\n",
    "    ),\n",
    "    (\n",
    "        \"Automation scripting\",\n",
    "        \"Writing scripts to automate operational processes.\",\n",
    "        category_id_map[\"DevOps\"],\n",
    "    ),\n",
    "    (\n",
    "        \"Infrastructure as Code\",\n",
    "        \"Managing infrastructure through code to improve deployment speed.\",\n",
    "        category_id_map[\"DevOps\"],\n",
    "    ),\n",
    "    (\n",
    "        \"Monitoring and logging\",\n",
    "        \"Tracking and analyzing system performance and activity.\",\n",
    "        category_id_map[\"DevOps\"],\n",
    "    ),\n",
    "    # Cloud Computing\n",
    "    (\n",
    "        \"Cloud Service Management\",\n",
    "        \"Managing cloud services and infrastructure.\",\n",
    "        category_id_map[\"Cloud Computing\"],\n",
    "    ),\n",
    "    (\n",
    "        \"Cloud Migration Strategies\",\n",
    "        \"Planning and executing the migration of services to cloud environments.\",\n",
    "        category_id_map[\"Cloud Computing\"],\n",
    "    ),\n",
    "    (\n",
    "        \"Cloud Security\",\n",
    "        \"Ensuring the security of cloud-based applications and data.\",\n",
    "        category_id_map[\"Cloud Computing\"],\n",
    "    ),\n",
    "    (\n",
    "        \"Cloud resource optimization\",\n",
    "        \"Managing cloud resources to optimize performance and cost.\",\n",
    "        category_id_map[\"Cloud Computing\"],\n",
    "    ),\n",
    "    # Front End Technologies\n",
    "    (\"Angular\", \"Angular development.\", category_id_map[\"Front End Technologies\"]),\n",
    "    (\"React\", \"React development.\", category_id_map[\"Front End Technologies\"]),\n",
    "    (\"Vue\", \"Vue.js development.\", category_id_map[\"Front End Technologies\"]),\n",
    "    (\n",
    "        \"HTML/CSS\",\n",
    "        \"Web development using HTML and CSS.\",\n",
    "        category_id_map[\"Front End Technologies\"],\n",
    "    ),\n",
    "    # Server Side Technologies\n",
    "    (\n",
    "        \"Node.js\",\n",
    "        \"Backend development with Node.js.\",\n",
    "        category_id_map[\"Server Side Technologies\"],\n",
    "    ),\n",
    "    (\n",
    "        \"Spring Boot\",\n",
    "        \"Application development with Java Spring Boot.\",\n",
    "        category_id_map[\"Server Side Technologies\"],\n",
    "    ),\n",
    "    (\n",
    "        \".NET\",\n",
    "        \".NET framework development.\",\n",
    "        category_id_map[\"Server Side Technologies\"],\n",
    "    ),\n",
    "    (\n",
    "        \"Laravel\",\n",
    "        \"Web development with Laravel.\",\n",
    "        category_id_map[\"Server Side Technologies\"],\n",
    "    ),\n",
    "    # Database Technologies\n",
    "    (\n",
    "        \"SQL\",\n",
    "        \"SQL query writing and optimization.\",\n",
    "        category_id_map[\"Database Technologies\"],\n",
    "    ),\n",
    "    (\"NoSQL\", \"NoSQL database management.\", category_id_map[\"Database Technologies\"]),\n",
    "    (\"DB Admin\", \"Database administration.\", category_id_map[\"Database Technologies\"]),\n",
    "    (\n",
    "        \"DB Design\",\n",
    "        \"Database design and normalization.\",\n",
    "        category_id_map[\"Database Technologies\"],\n",
    "    ),\n",
    "    # Business Intelligence\n",
    "    (\n",
    "        \"Power BI\",\n",
    "        \"Business intelligence with Power BI.\",\n",
    "        category_id_map[\"Business Intelligence\"],\n",
    "    ),\n",
    "    (\n",
    "        \"Tableau\",\n",
    "        \"Data visualization with Tableau.\",\n",
    "        category_id_map[\"Business Intelligence\"],\n",
    "    ),\n",
    "    (\n",
    "        \"Apache Superset\",\n",
    "        \"Business intelligence with Apache Superset.\",\n",
    "        category_id_map[\"Business Intelligence\"],\n",
    "    ),\n",
    "    (\n",
    "        \"BI Tools\",\n",
    "        \"Using various business intelligence tools.\",\n",
    "        category_id_map[\"Business Intelligence\"],\n",
    "    ),\n",
    "    # User Interface Design\n",
    "    (\n",
    "        \"UX/UI Design\",\n",
    "        \"User experience and user interface design.\",\n",
    "        category_id_map[\"User Interface Design\"],\n",
    "    ),\n",
    "    (\n",
    "        \"Web Design\",\n",
    "        \"Designing responsive web layouts.\",\n",
    "        category_id_map[\"User Interface Design\"],\n",
    "    ),\n",
    "    (\n",
    "        \"Graphic Design\",\n",
    "        \"Graphic design for web and print.\",\n",
    "        category_id_map[\"User Interface Design\"],\n",
    "    ),\n",
    "    (\n",
    "        \"Interaction Design\",\n",
    "        \"Designing interactive user interfaces.\",\n",
    "        category_id_map[\"User Interface Design\"],\n",
    "    ),\n",
    "    # Mobile Development\n",
    "    (\n",
    "        \"Flutter\",\n",
    "        \"Mobile app development with Flutter.\",\n",
    "        category_id_map[\"Mobile Development\"],\n",
    "    ),\n",
    "    (\n",
    "        \"React Native\",\n",
    "        \"Building mobile apps with React Native.\",\n",
    "        category_id_map[\"Mobile Development\"],\n",
    "    ),\n",
    "    (\"iOS\", \"Developing apps for iOS.\", category_id_map[\"Mobile Development\"]),\n",
    "    (\"Android\", \"Developing apps for Android.\", category_id_map[\"Mobile Development\"]),\n",
    "    # Web Scraping\n",
    "    (\"Selenium\", \"Web scraping with Selenium.\", category_id_map[\"Web Scraping\"]),\n",
    "    (\n",
    "        \"Puppeteer\",\n",
    "        \"Automated browser control with Puppeteer.\",\n",
    "        category_id_map[\"Web Scraping\"],\n",
    "    ),\n",
    "    (\n",
    "        \"Web Crawl\",\n",
    "        \"Crawling websites for data extraction.\",\n",
    "        category_id_map[\"Web Scraping\"],\n",
    "    ),\n",
    "    (\n",
    "        \"Data Scraping\",\n",
    "        \"Extracting data from web sources.\",\n",
    "        category_id_map[\"Web Scraping\"],\n",
    "    ),\n",
    "    # Scripting Languages\n",
    "    (\"Python\", \"Scripting with Python.\", category_id_map[\"Scripting Languages\"]),\n",
    "    (\"Bash\", \"Shell scripting with Bash.\", category_id_map[\"Scripting Languages\"]),\n",
    "    (\"Ruby\", \"Scripting with Ruby.\", category_id_map[\"Scripting Languages\"]),\n",
    "    (\n",
    "        \"PowerShell\",\n",
    "        \"Automation with PowerShell.\",\n",
    "        category_id_map[\"Scripting Languages\"],\n",
    "    ),\n",
    "    # ELT / ETL Technologies\n",
    "    (\"DBT\", \"Data transformation with DBT.\", category_id_map[\"ELT / ETL Technologies\"]),\n",
    "    (\n",
    "        \"Alteryx\",\n",
    "        \"Data processing with Alteryx.\",\n",
    "        category_id_map[\"ELT / ETL Technologies\"],\n",
    "    ),\n",
    "    (\n",
    "        \"Azure Synapse\",\n",
    "        \"Data warehousing with Azure Synapse.\",\n",
    "        category_id_map[\"ELT / ETL Technologies\"],\n",
    "    ),\n",
    "    (\n",
    "        \"Databricks\",\n",
    "        \"Big data processing with Databricks.\",\n",
    "        category_id_map[\"ELT / ETL Technologies\"],\n",
    "    ),\n",
    "    # Data Warehousing\n",
    "    (\n",
    "        \"ETL Development\",\n",
    "        \"Building Extract, Transform, Load processes for data warehousing.\",\n",
    "        category_id_map[\"Data Warehousing\"],\n",
    "    ),\n",
    "    (\n",
    "        \"Data Modeling\",\n",
    "        \"Designing data models for efficient storage and retrieval.\",\n",
    "        category_id_map[\"Data Warehousing\"],\n",
    "    ),\n",
    "    (\n",
    "        \"Data Integration\",\n",
    "        \"Integrating data from various sources into a data warehouse.\",\n",
    "        category_id_map[\"Data Warehousing\"],\n",
    "    ),\n",
    "    (\n",
    "        \"Data Warehouse Optimization\",\n",
    "        \"Optimizing performance and storage in data warehousing systems.\",\n",
    "        category_id_map[\"Data Warehousing\"],\n",
    "    ),\n",
    "    # Machine Learning\n",
    "    (\n",
    "        \"Deep Learning\",\n",
    "        \"Applying neural networks for complex pattern recognition.\",\n",
    "        category_id_map[\"Machine Learning\"],\n",
    "    ),\n",
    "    (\n",
    "        \"Reinforcement Learning\",\n",
    "        \"Developing AI agents that learn through interactions.\",\n",
    "        category_id_map[\"Machine Learning\"],\n",
    "    ),\n",
    "    (\n",
    "        \"Model Deployment\",\n",
    "        \"Deploying machine learning models for real-world applications.\",\n",
    "        category_id_map[\"Machine Learning\"],\n",
    "    ),\n",
    "    (\n",
    "        \"Model Interpretability\",\n",
    "        \"Understanding and explaining the decisions made by machine learning models.\",\n",
    "        category_id_map[\"Machine Learning\"],\n",
    "    ),\n",
    "    # Natural Language Processing\n",
    "    (\n",
    "        \"Named Entity Recognition (NER)\",\n",
    "        \"Identifying and classifying named entities in text data.\",\n",
    "        category_id_map[\"Natural Language Processing\"],\n",
    "    ),\n",
    "    (\n",
    "        \"Text Classification\",\n",
    "        \"Categorizing text into predefined classes or categories.\",\n",
    "        category_id_map[\"Natural Language Processing\"],\n",
    "    ),\n",
    "    (\n",
    "        \"Language Translation\",\n",
    "        \"Translating text between different languages.\",\n",
    "        category_id_map[\"Natural Language Processing\"],\n",
    "    ),\n",
    "    (\n",
    "        \"Sentiment Analysis\",\n",
    "        \"Analyzing text to determine sentiment or emotional tone.\",\n",
    "        category_id_map[\"Natural Language Processing\"],\n",
    "    ),\n",
    "    # Cloud Operations\n",
    "    (\n",
    "        \"AWS Services Management\",\n",
    "        \"Managing AWS services for cloud-based operations.\",\n",
    "        category_id_map[\"Cloud Operations\"],\n",
    "    ),\n",
    "    (\n",
    "        \"Google Cloud Platform (GCP) Operations\",\n",
    "        \"Handling cloud operations on the Google Cloud Platform.\",\n",
    "        category_id_map[\"Cloud Operations\"],\n",
    "    ),\n",
    "    (\n",
    "        \"Kubernetes Administration\",\n",
    "        \"Administering Kubernetes clusters for container orchestration.\",\n",
    "        category_id_map[\"Cloud Operations\"],\n",
    "    ),\n",
    "    (\n",
    "        \"Cloud Cost Management\",\n",
    "        \"Optimizing cloud usage and costs for efficient operations.\",\n",
    "        category_id_map[\"Cloud Operations\"],\n",
    "    ),\n",
    "]\n",
    "\n",
    "faker = Faker()\n",
    "\n",
    "org_id = ORG_ID\n",
    "\n",
    "\n",
    "# Generate Skill entries with Faker-generated IDs\n",
    "skills = [\n",
    "    {\n",
    "        \"id\": i,  # Auto-incrementing ID, padded to 5 digits\n",
    "        \"name\": name,\n",
    "        \"description\": description,\n",
    "        \"org_id\": org_id,\n",
    "        \"skill_category_id\": skill_category_id,\n",
    "    }\n",
    "    for i, (name, description, skill_category_id) in enumerate(skills_info, start=1)\n",
    "]\n",
    "\n",
    "# Convert to JSON string for output\n",
    "skills_json = json.dumps(skills, indent=2)\n",
    "\n",
    "# Writing the skills data to a JSON file\n",
    "skills_file_path = \"data/skills.json\"\n",
    "with open(skills_file_path, \"w\") as file:\n",
    "    file.write(skills_json)\n",
    "\n",
    "skills_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Inserting Roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/roles.json'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import uuid\n",
    "import json\n",
    "\n",
    "def generate_role_data_json():\n",
    "    organization_id = ORG_ID\n",
    "    role_hierarchy = {\n",
    "        \"Chief Information Officer\": {\n",
    "            \"description\": \"Oversees the IT strategy and operations.\",\n",
    "            \"children\": [\"IT Manager\", \"Cybersecurity Manager\", \"Cloud Solutions Architect\"]\n",
    "        },\n",
    "        \"IT Manager\": {\n",
    "            \"description\": \"Manages the IT department and its resources.\",\n",
    "            \"children\": [\"Development Manager\", \"Database Administrator\", \"Project Manager\", \"Network Administrator\"]\n",
    "        },\n",
    "        \"Development Manager\": {\n",
    "            \"description\": \"Oversees software development teams and projects.\",\n",
    "            \"children\": [\"Front-End Developer\", \"Back-End Developer\"]\n",
    "        },\n",
    "        \"Cybersecurity Manager\": {\n",
    "            \"description\": \"Oversees cybersecurity policies and defenses.\",\n",
    "            \"children\": [\"Cybersecurity Analyst\"]\n",
    "        },\n",
    "        \"Cloud Solutions Architect\": {\n",
    "            \"description\": \"Designs and manages cloud solutions.\",\n",
    "            \"children\": []\n",
    "        },\n",
    "        \"Database Administrator\": {\n",
    "            \"description\": \"Manages the organization's database systems.\",\n",
    "            \"children\": []\n",
    "        },\n",
    "        \"Project Manager\": {\n",
    "            \"description\": \"Manages projects from inception to completion.\",\n",
    "            \"children\": [\"DevOps Engineer\"]\n",
    "        },\n",
    "        \"DevOps Engineer\": {\n",
    "            \"description\": \"Improves operations between software development and IT operations.\",\n",
    "            \"children\": []\n",
    "        },\n",
    "        \"Data Scientist\": {\n",
    "            \"description\": \"Analyzes data to extract insights.\",\n",
    "            \"children\": []\n",
    "        },\n",
    "        \"Cybersecurity Analyst\": {\n",
    "            \"description\": \"Protects against cyber threats.\",\n",
    "            \"children\": []\n",
    "        },\n",
    "        \"Front-End Developer\": {\n",
    "            \"description\": \"Develops the user interface of web applications.\",\n",
    "            \"children\": []\n",
    "        },\n",
    "        \"Back-End Developer\": {\n",
    "            \"description\": \"Develops the server-side logic of applications.\",\n",
    "            \"children\": []\n",
    "        },\n",
    "        \"Business Analyst\": {\n",
    "            \"description\": \"Analyzes the business processes and recommends solutions.\",\n",
    "            \"children\": []\n",
    "        },\n",
    "        \"Network Administrator\": {\n",
    "            \"description\": \"Manages and troubleshoots the organization's network infrastructure.\",\n",
    "            \"children\": []\n",
    "        }\n",
    "    }\n",
    "\n",
    "    roles = []\n",
    "    parent_map = {}\n",
    "    role_id = 1\n",
    "\n",
    "    def add_role(name, description, parent_id=None):\n",
    "        nonlocal role_id\n",
    "        \n",
    "        roles.append({\n",
    "            \"id\": role_id,\n",
    "            \"name\": name,\n",
    "            \"description\": description,\n",
    "            \"org_id\": organization_id,\n",
    "            \"parent_id\": parent_id\n",
    "        })\n",
    "        parent_map[name] = role_id\n",
    "        role_id += 1\n",
    "\n",
    "    def add_roles_recursively(name, parent_id=None):\n",
    "        info = role_hierarchy[name]\n",
    "        add_role(name, info['description'], parent_id)\n",
    "        for child_name in info['children']:\n",
    "            add_roles_recursively(child_name, parent_map[name])\n",
    "\n",
    "    add_roles_recursively(\"Chief Information Officer\")\n",
    "\n",
    "    return json.dumps(roles, indent=4)\n",
    "role_data_file_path = f\"{folder_name}/roles.json\"\n",
    "with open(role_data_file_path, 'w') as file:\n",
    "    file.write(generate_role_data_json())\n",
    "\n",
    "role_data_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Inserting Roles Skills Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corporate governance\n",
      "Strategic planning\n",
      "Visionary leadership\n",
      "Financial acumen\n",
      "Network security\n",
      "Information security\n",
      "Cybersecurity policies\n",
      "Incident response\n",
      "System design\n",
      "Coding best practices\n",
      "Software testing\n",
      "Continuous integration\n",
      "Resource allocation\n",
      "Risk assessment\n",
      "Project scheduling\n",
      "Stakeholder management\n",
      "Data Analysis\n",
      "Machine Learning\n",
      "Data visualization\n",
      "Big Data technologies\n",
      "Continuous Integration/Continuous Deployment\n",
      "Automation scripting\n",
      "Infrastructure as Code\n",
      "Monitoring and logging\n",
      "Cloud Service Management\n",
      "Cloud Migration Strategies\n",
      "Cloud Security\n",
      "Cloud resource optimization\n",
      "Angular\n",
      "React\n",
      "Vue\n",
      "HTML/CSS\n",
      "Node.js\n",
      "Spring Boot\n",
      ".NET\n",
      "Laravel\n",
      "SQL\n",
      "NoSQL\n",
      "DB Admin\n",
      "DB Design\n",
      "Power BI\n",
      "Tableau\n",
      "Apache Superset\n",
      "BI Tools\n",
      "UX/UI Design\n",
      "Web Design\n",
      "Graphic Design\n",
      "Interaction Design\n",
      "Flutter\n",
      "React Native\n",
      "iOS\n",
      "Android\n",
      "Selenium\n",
      "Puppeteer\n",
      "Web Crawl\n",
      "Data Scraping\n",
      "Python\n",
      "Bash\n",
      "Ruby\n",
      "PowerShell\n",
      "DBT\n",
      "Alteryx\n",
      "Azure Synapse\n",
      "Databricks\n",
      "ETL Development\n",
      "Data Modeling\n",
      "Data Integration\n",
      "Data Warehouse Optimization\n",
      "Deep Learning\n",
      "Reinforcement Learning\n",
      "Model Deployment\n",
      "Model Interpretability\n",
      "Named Entity Recognition (NER)\n",
      "Text Classification\n",
      "Language Translation\n",
      "Sentiment Analysis\n",
      "AWS Services Management\n",
      "Google Cloud Platform (GCP) Operations\n",
      "Kubernetes Administration\n",
      "Cloud Cost Management\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Inserting Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "import bcrypt\n",
    "import uuid\n",
    "import json\n",
    "\n",
    "# Generate fake data\n",
    "fake = Faker()\n",
    "\n",
    "# Define role capacities\n",
    "role_capacities = {\n",
    "    \"Chief Information Officer\": 1,\n",
    "    \"IT Manager\": 5,\n",
    "    \"Front-End Developer\": 8,\n",
    "    \"Back-End Developer\": 6,\n",
    "    \"Database Administrator\": 4,\n",
    "    \"Cybersecurity Manager\": 2,\n",
    "    \"Cybersecurity Analyst\": 10,\n",
    "    \"Cloud Solutions Architect\": 3,\n",
    "}\n",
    "\n",
    "# Define security groups\n",
    "security_groups = [\"admin\", \"supervisor\", \"general staff\"]\n",
    "\n",
    "# Load role data from JSON\n",
    "with open(\"data/roles.json\", \"r\") as file:\n",
    "    roles_data = json.load(file)\n",
    "\n",
    "# Generate users\n",
    "users = []\n",
    "for role, capacity in role_capacities.items():\n",
    "    role_id = None\n",
    "    for item in roles_data:\n",
    "        if item[\"name\"] == role:\n",
    "            role_id = item[\"id\"]\n",
    "            break\n",
    "\n",
    "    if role_id is not None:\n",
    "        for _ in range(capacity):\n",
    "            first_name = fake.first_name()\n",
    "            last_name = fake.last_name()\n",
    "            email = fake.email()\n",
    "\n",
    "            # Hash password using bcrypt\n",
    "            password = bcrypt.hashpw(b\"Pass@1234\", bcrypt.gensalt()).decode(\"utf-8\")\n",
    "\n",
    "            # Generate user entry\n",
    "            user_entry = {\n",
    "                \"id\": str(uuid.uuid4()),\n",
    "                \"password\": password,\n",
    "                \"org_id\": \"8fd51c1d-32f9-48e3-a473-cf57f29bea05\",\n",
    "                \"firstName\": first_name,\n",
    "                \"lastName\": last_name,\n",
    "                \"emailAddress\": email,\n",
    "                \"roleId\": role_id,\n",
    "                \"locationId\": None,  # You can assign locations if needed\n",
    "                \"securityGroup\": fake.random_element(security_groups),\n",
    "            }\n",
    "            users.append(user_entry)\n",
    "\n",
    "# Save users to JSON file\n",
    "users_file_path = \"data/users.json\"\n",
    "with open(users_file_path, \"w\") as file:\n",
    "    json.dump(users, file, indent=4)\n",
    "\n",
    "users_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Inserting Users & Skills Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Inserting Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Skill Audit Log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Evidence Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. General Evidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Training Evidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
